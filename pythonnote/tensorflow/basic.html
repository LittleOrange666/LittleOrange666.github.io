<html>

<head>
    <title>python tensorflow 基礎</title>
    <base target="_top">
    <link href="https://littleorange666.github.io/style/codehilite.css" rel="stylesheet">
    <link href="https://littleorange666.github.io/style/main.css" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-wEmeIV1mKuiNpC+IOBjI7aAzPcEZeedi5yW5f2yOq55WWLwNGmvvx4Um1vskeMj0" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0/dist/js/bootstrap.bundle.min.js" integrity="sha384-p34f1UUtsS3wqzfto5wAAmdvj+osOnFyQFpp4Ua3gs/ZVWx6oOypYoCJhGGScy+8" crossorigin="anonymous"></script>
    <script src="https://littleorange666.github.io/script/jquery.js"></script>
</head>

<body>
    <div id="top_area">
        <nav class="navbar navbar-expand-lg navbar-light bg-light">
            <div class="container-fluid">
                <ul class="navbar-nav me-auto mb-2 mb-lg-0">
                    <li class="nav-item">
                        <a class="nav-link active" aria-current="page" href="https://littleorange666.github.io/">首頁</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link active" aria-current="page" href="#" id="previous">前一頁</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link active" aria-current="page" href="#" id="previous">回到頂部</a>
                    </li>
                </ul>
            </div>
        </nav>
    </div>
    <div id="indexbar" class="toc"></div>
    <div id="main_area" class="container-fluid" data-hard-breaks="true">
        <h1 id="tensorflow">tensorflow 基礎</h1>
<p>先import<br>
因為和通常numpy、matplotlib一起用，所以開頭要寫</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">'font.sans-serif'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Microsoft JhengHei'</span><span class="p">]</span> <span class="c1"># 正常顯示中文</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">'axes.unicode_minus'</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># 正常顯示負號</span>
</code></pre></div>

<h2 id="_1">基本部分</h2>
<h3 id="_2">常用資料型別</h3>
<h4 id="tftensor">張量 tf.Tensor</h4>
<p>基本有點像np.ndarray<br>
但可以對它進行操作(tf.Operation)但不立即執行(在tensorflow 1)<br>
在tensorflow 2中可用tf.Tensor::numpy()轉換成np.ndarray</p>
<h4 id="tfvariable">變數 tf.Variable</h4>
<p>此類別用以維護一個張量，可以對其進行操作、讀寫，通常用於紀錄模型參數</p>
<h4 id="tfoperation">操作 tf.Operation</h4>
<p>用於對張量/變數/Session進行操作</p>
<h3 id="_3">常用張量計算</h3>
<h4 id="_4">加/減</h4>
<p>直接用加號/減號</p>
<h4 id="_5">對應值相乘</h4>
<p>用</p>
<div class="codehilite"><pre><span></span><code><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">)</span>
</code></pre></div>

<h4 id="_6">矩陣相乘</h4>
<p>用</p>
<div class="codehilite"><pre><span></span><code><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">)</span>
</code></pre></div>

<h4 id="_7">平方</h4>
<p>用</p>
<div class="codehilite"><pre><span></span><code><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">)</span>
</code></pre></div>

<h4 id="_8">根號</h4>
<p>用</p>
<div class="codehilite"><pre><span></span><code><span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">)</span>
</code></pre></div>

<h4 id="_9">自定義</h4>
<p>除了標準的張量計算外，還可以用<code>@tf.function</code>來建立自定義的張量計算</p>
<h3 id="eager-execution">關於eager execution</h3>
<p>eager execution表示所有計算都立即執行<br>
在tensorflow 2默認是啟用的<br>
可用以下任一行來禁用</p>
<div class="codehilite"><pre><span></span><code><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">disable_v2_behavior</span><span class="p">()</span> <span class="c1"># 禁用tensorflow 2全域特性防止出RuntimeError</span>
<span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">disable_eager_execution</span><span class="p">()</span> <span class="c1"># 禁用eager execution防止出RuntimeError</span>
</code></pre></div>

<p>若禁用eager execution，則計算結果必須通過Session::run來讀取<br>
但可以利用占位符作計算，把資料留到輸出結果時再輸入</p>
<h2 id="_10">架構</h2>
<p>tensorflow中有5種不同的架構<br>
+ 靜態圖：最原始也最靈活的架構<br>
+ 動態圖：比起靜態圖更符合Python的編寫方式<br>
+ 估算器：整合了常用功能的進階API<br>
+ keras：十分通用的前端神經網路介面<br>
+ swift</p>
<h3 id="_11">靜態圖</h3>
<p>需禁用eager execution<br>
靜態圖將"定義"與"執行"分離<br>
預先定義好架構與損失後，建立Session進行訓練</p>
<h4 id="_12">靜態圖架構</h4>
<p>靜態圖的架構包含占位符、模型參數、損失函數和梯度下降器</p>
<ul>
<li>占位符: tf.Tensor<br>
  占位符表示模型中可輸入資料的空位<br>
  語法：</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">名稱</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">"float"</span><span class="p">)</span>
</code></pre></div>

<ul>
<li>模型參數: tf.Variable<br>
  模型參數是模型中要透過訓練求得的值<br>
  語法：</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">名稱</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">初始值</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">名稱</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span>
</code></pre></div>

<ul>
<li>
<p>損失函數: tf.Tensor<br>
  損失函數是一個用於判斷正確程度的值，應為一個透過對占位符及模型參數作數學計算後的到的張量</p>
</li>
<li>
<p>梯度下降器: tf.Operation<br>
  梯度下降器是用於反向修正的算法，通常不會自己寫<br>
  語法：</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'global_step'</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">損失函數</span><span class="p">,</span> <span class="n">global_step</span><span class="p">)</span>
</code></pre></div>

<h4 id="session">Session建立與使用</h4>
<p>主要訓練部分需要建立一個Session<br>
語法：</p>
<div class="codehilite"><pre><span></span><code><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span> <span class="c1"># 初始化變數</span>
    <span class="c1"># 主程式</span>
</code></pre></div>

<p>sess.run有以下功能：</p>
<ul>
<li>sess.run(Operation) 進行操作</li>
<li>sess.run(Tensor|Variable) 取出值</li>
</ul>
<p>使用<code>sess.run(optimizer)</code>可進行訓練<br>
使用<code>sess.run(損失函數)</code>可求出損失值<br>
這兩個操作都需要輸入資料<br>
使用<code>sess.run(模型參數)</code>可取出模型參數</p>
<h4 id="_13">範例</h4>
<p><details><summary>範例：y=ax+b</p></summary>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">'font.sans-serif'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Microsoft JhengHei'</span><span class="p">]</span> <span class="c1"># 正常顯示中文</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">'axes.unicode_minus'</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># 正常顯示負號</span>
<span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">disable_eager_execution</span><span class="p">()</span> <span class="c1"># 禁用eager execution防止出RuntimeError</span>
<span class="c1">#隨機參數</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="n">random_a</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">random_b</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="c1"># 產生資料</span>
<span class="n">train_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">train_Y</span> <span class="o">=</span> <span class="n">random_a</span> <span class="o">*</span> <span class="n">train_X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">train_X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">random_b</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">train_X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">random_a</span>  <span class="c1"># y=ax+b，但是加入了噪聲</span>
<span class="c1"># 輸入值</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">"float"</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">"float"</span><span class="p">)</span>
<span class="c1"># 可訓練參數</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s2">"weight"</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s2">"bias"</span><span class="p">)</span>
<span class="c1"># 前嚮結構</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'global_step'</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># 反向改善</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">z</span><span class="p">))</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">global_step</span><span class="p">)</span>  <span class="c1"># 梯度下降</span>
<span class="c1"># 初始化所有變數</span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="c1"># 定義訓練參數</span>
<span class="n">training_epochs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">display_step</span> <span class="o">=</span> <span class="mi">2</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
    <span class="k">while</span> <span class="n">global_step</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_X</span><span class="p">)</span> <span class="o"><</span> <span class="n">training_epochs</span><span class="p">:</span>  <span class="c1"># step=執行次數/資料大小=global_step.eval()/len(train_X)</span>
        <span class="n">step</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">global_step</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_X</span><span class="p">))</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">):</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">y</span><span class="p">})</span>  <span class="c1"># 餵資料</span>
        <span class="c1"># 顯示訓練中的詳細訊息</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">display_step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">train_X</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">train_Y</span><span class="p">})</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"Epoch:"</span><span class="p">,</span> <span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"cost="</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s2">"W="</span><span class="p">,</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">W</span><span class="p">),</span> <span class="s2">"b="</span><span class="p">,</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Finished!"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"cost="</span><span class="p">,</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">train_X</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">train_Y</span><span class="p">}),</span> <span class="s2">"W="</span><span class="p">,</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">W</span><span class="p">),</span> <span class="s2">"b="</span><span class="p">,</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
    <span class="c1"># 顯示模型</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="s1">'ro'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Original data'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">W</span><span class="p">)</span> <span class="o">*</span> <span class="n">train_X</span> <span class="o">+</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Fitted line'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p></details></p>
<h3 id="_14">動態圖</h3>
<p>動態圖基於eager execution<br>
不需要建立Session<br>
直接計算梯度並更改參數</p>
<h4 id="_15">動態圖架構</h4>
<p>動態圖架構包含模型參數、損失函數、改善器</p>
<ul>
<li>模型參數: tf.Variable<br>
  模型參數是模型中要透過訓練求得的值</li>
<li>損失函數: tf.Tensor<br>
  損失函數是一個用於判斷正確程度的值，應為一個透過對占位符及模型參數作數學計算後的到的張量<br>
  與靜態圖不同的是，損失函數是動態計算而不是在一開始就固定</li>
<li>改善器: GradientDescentOptimizer<br>
  改善器能夠透過梯度自動優化參數<br>
  語法：</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</code></pre></div>

<h4 id="_16">動態圖使用</h4>
<p>首先定義好模型參數與改善器<br>
並在過程中計算梯度<br>
<strong>計算梯度</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">cost_</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">損失函數</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">cost_</span><span class="p">,</span> <span class="n">sources</span><span class="o">=</span><span class="p">[</span><span class="n">模型參數列表</span><span class="p">])</span>  <span class="c1"># 計算梯度</span>
</code></pre></div>

<p><strong>優化參數</strong></p>
<div class="codehilite"><pre><span></span><code><span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="p">[</span><span class="n">模型參數列表</span><span class="p">]),</span> <span class="n">global_step</span><span class="p">)</span>
</code></pre></div>

<p>要取出任何的模型參數可直接使用</p>
<div class="codehilite"><pre><span></span><code><span class="n">參數</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div>

<p><details><summary>範例：y=ax+b</p></summary>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">'font.sans-serif'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Microsoft JhengHei'</span><span class="p">]</span> <span class="c1"># 正常顯示中文</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">'axes.unicode_minus'</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># 正常顯示負號</span>
<span class="c1">#隨機參數</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="n">random_a</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">random_b</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="c1"># 產生資料</span>
<span class="n">train_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">train_Y</span> <span class="o">=</span> <span class="n">random_a</span> <span class="o">*</span> <span class="n">train_X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">train_X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">random_b</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">train_X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">random_a</span>  <span class="c1"># y=ax+b，但是加入了噪聲</span>
<span class="c1"># 定義訓練參數</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s2">"weight"</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s2">"bias"</span><span class="p">)</span>
<span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'global_step'</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">getcost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>  <span class="c1"># 定義函數，計算loss值</span>
    <span class="c1"># 前嚮結構</span>
    <span class="c1">#z = tf.cast(tf.multiply(np.asarray(x, dtype=np.float32), W) + b, dtype=tf.float32)</span>
    <span class="n">z</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">))</span>  <span class="c1"># loss值</span>
    <span class="k">return</span> <span class="n">cost</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="c1"># 隨機梯度下降法作為改善器</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">training_epochs</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># 迭代訓練次數</span>
<span class="n">display_step</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">old_step</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="k">while</span> <span class="n">global_step</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_X</span><span class="p">)</span> <span class="o"><</span> <span class="n">training_epochs</span><span class="p">:</span>  <span class="c1"># 迭代訓練模型</span>
    <span class="n">step</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">global_step</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_X</span><span class="p">))</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">cost_</span> <span class="o">=</span> <span class="n">getcost</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">)</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">cost_</span><span class="p">,</span> <span class="n">sources</span><span class="o">=</span><span class="p">[</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>  <span class="c1"># 計算梯度</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="p">[</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">]),</span> <span class="n">global_step</span><span class="p">)</span>
    <span class="c1"># 顯示訓練中的詳細訊息</span>
    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">display_step</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">step</span><span class="o">!=</span><span class="n">old_step</span><span class="p">:</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">cost_</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># 損失值換為np.ndarray</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Epoch:"</span><span class="p">,</span> <span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"cost="</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="s2">"W="</span><span class="p">,</span> <span class="n">W</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s2">"b="</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">old_step</span><span class="o">=</span><span class="n">step</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">" Finished!"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"cost="</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="s2">"W="</span><span class="p">,</span> <span class="n">W</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s2">"b="</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="s1">'ro'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Original data'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">W</span> <span class="o">*</span> <span class="n">train_X</span> <span class="o">+</span> <span class="n">b</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Fitted line'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p></details></p>
<h3 id="_17">估算器</h3>
<h3 id="keras">keras</h3>
<p>詳見<a href="keras">keras</a></p>
    </div>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            config: ["MMLorHTML.js"],
            jax: ["input/TeX", "output/HTML-CSS", "output/NativeMML"],
            extensions: ["MathMenu.js", "MathZoom.js"],
            enable_dollar_delimiter: true
        });
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
    <script src="https://littleorange666.github.io/script/in_view.js"></script>
    <script src="https://littleorange666.github.io/script/main.js"></script>
</body>

</html>