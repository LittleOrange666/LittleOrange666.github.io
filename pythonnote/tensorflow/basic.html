<html>

<head>
    <title>python tensorflow 基礎</title>
    <base target="_top">
    <link href="https://littleorange666.github.io/style/codehilite.css" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-wEmeIV1mKuiNpC+IOBjI7aAzPcEZeedi5yW5f2yOq55WWLwNGmvvx4Um1vskeMj0" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0/dist/js/bootstrap.bundle.min.js" integrity="sha384-p34f1UUtsS3wqzfto5wAAmdvj+osOnFyQFpp4Ua3gs/ZVWx6oOypYoCJhGGScy+8" crossorigin="anonymous"></script>
    <style type="text/css">
    #top_area {
        position: fixed;
        left: 0px;
        right: 0px;
    }

    #main_area {
        width: 80%;
        padding-top: 50px;
    }
    </style>
</head>

<body>
    <div id="top_area">
        <nav class="navbar navbar-expand-lg navbar-light bg-light">
            <div class="container-fluid">
                <a class="navbar-brand" href="#"></a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse" id="navbarSupportedContent">
                    <ul class="navbar-nav me-auto mb-2 mb-lg-0">
                        <li class="nav-item">
                            <a class="nav-link active" aria-current="page" href="https://littleorange666.github.io/">首頁</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link active" aria-current="page" href="#" id="previous">前一頁</a>
                        </li>
                </div>
            </div>
        </nav>
    </div>
    <div id="main_area" class="container-fluid" data-hard-breaks="true">
        <h1 id="header-1" class="a_header a_content" data-connectedheader="header-1">tensorflow 基礎</h1>
<p data-connectedheader="header-1" class="a_content">先import<br>
因為和通常numpy、matplotlib一起用，所以開頭要寫</p>
<div class="codehilite"><pre data-connectedheader="header-1" class="a_content"><span></span><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">'font.sans-serif'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Microsoft JhengHei'</span><span class="p">]</span> <span class="c1"># 正常顯示中文</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">'axes.unicode_minus'</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># 正常顯示負號</span>
</code></pre></div>

<h2 id="header-2" class="a_header a_content" data-connectedheader="header-2">基本部分</h2>
<h3 id="header-3" class="a_header a_content" data-connectedheader="header-3">常用資料型別</h3>
<h4 id="tftensor" data-connectedheader="header-3" class="a_content">張量 tf.Tensor</h4>
<p data-connectedheader="header-3" class="a_content">基本有點像np.ndarray<br>
但可以對它進行操作(tf.Operation)但不立即執行(在tensorflow 1)<br>
在tensorflow 2中可用tf.Tensor::numpy()轉換成np.ndarray</p>
<h4 id="tfvariable" data-connectedheader="header-3" class="a_content">變數 tf.Variable</h4>
<p data-connectedheader="header-3" class="a_content">此類別用以維護一個張量，可以對其進行操作、讀寫，通常用於紀錄模型參數</p>
<h4 id="tfoperation" data-connectedheader="header-3" class="a_content">操作 tf.Operation</h4>
<p data-connectedheader="header-3" class="a_content">用於對張量/變數/Session進行操作</p>
<h3 id="header-4" class="a_header a_content" data-connectedheader="header-4">常用張量計算</h3>
<h4 id="_4" data-connectedheader="header-4" class="a_content">加/減</h4>
<p data-connectedheader="header-4" class="a_content">直接用加號/減號</p>
<h4 id="_5" data-connectedheader="header-4" class="a_content">對應值相乘</h4>
<p data-connectedheader="header-4" class="a_content">用</p>
<div class="codehilite"><pre data-connectedheader="header-4" class="a_content"><span></span><code><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">)</span>
</code></pre></div>

<h4 id="_6" data-connectedheader="header-4" class="a_content">矩陣相乘</h4>
<p data-connectedheader="header-4" class="a_content">用</p>
<div class="codehilite"><pre data-connectedheader="header-4" class="a_content"><span></span><code><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">)</span>
</code></pre></div>

<h4 id="_7" data-connectedheader="header-4" class="a_content">平方</h4>
<p data-connectedheader="header-4" class="a_content">用</p>
<div class="codehilite"><pre data-connectedheader="header-4" class="a_content"><span></span><code><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">)</span>
</code></pre></div>

<h4 id="_8" data-connectedheader="header-4" class="a_content">根號</h4>
<p data-connectedheader="header-4" class="a_content">用</p>
<div class="codehilite"><pre data-connectedheader="header-4" class="a_content"><span></span><code><span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">)</span>
</code></pre></div>

<h4 id="_9" data-connectedheader="header-4" class="a_content">自定義</h4>
<p data-connectedheader="header-4" class="a_content">除了標準的張量計算外，還可以用<code>@tf.function</code>來建立自定義的張量計算</p>
<h3 id="header-5" class="a_header a_content" data-connectedheader="header-5">關於eager execution</h3>
<p data-connectedheader="header-5" class="a_content">eager execution表示所有計算都立即執行<br>
在tensorflow 2默認是啟用的<br>
可用以下任一行來禁用</p>
<div class="codehilite"><pre data-connectedheader="header-5" class="a_content"><span></span><code><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">disable_v2_behavior</span><span class="p">()</span> <span class="c1"># 禁用tensorflow 2全域特性防止出RuntimeError</span>
<span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">disable_eager_execution</span><span class="p">()</span> <span class="c1"># 禁用eager execution防止出RuntimeError</span>
</code></pre></div>

<p data-connectedheader="header-5" class="a_content">若禁用eager execution，則計算結果必須通過Session::run來讀取<br>
但可以利用占位符作計算，把資料留到輸出結果時再輸入</p>
<h2 id="header-6" class="a_header a_content" data-connectedheader="header-6">架構</h2>
<p data-connectedheader="header-6" class="a_content">tensorflow中有5種不同的架構<br>
+ 靜態圖：最原始也最靈活的架構<br>
+ 動態圖：比起靜態圖更符合Python的編寫方式<br>
+ 估算器：整合了常用功能的進階API<br>
+ keras：十分通用的前端神經網路介面<br>
+ swift</p>
<h3 id="header-7" class="a_header a_content" data-connectedheader="header-7">靜態圖</h3>
<p data-connectedheader="header-7" class="a_content">需禁用eager execution<br>
靜態圖將"定義"與"執行"分離<br>
預先定義好架構與損失後，建立Session進行訓練</p>
<h4 id="_12" data-connectedheader="header-7" class="a_content">靜態圖架構</h4>
<p data-connectedheader="header-7" class="a_content">靜態圖的架構包含占位符、模型參數、損失函數和梯度下降器</p>
<ul data-connectedheader="header-7" class="a_content">
<li>占位符: tf.Tensor<br>
  占位符表示模型中可輸入資料的空位<br>
  語法：</li>
</ul>
<div class="codehilite"><pre data-connectedheader="header-7" class="a_content"><span></span><code><span class="n">名稱</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">"float"</span><span class="p">)</span>
</code></pre></div>

<ul data-connectedheader="header-7" class="a_content">
<li>模型參數: tf.Variable<br>
  模型參數是模型中要透過訓練求得的值<br>
  語法：</li>
</ul>
<div class="codehilite"><pre data-connectedheader="header-7" class="a_content"><span></span><code><span class="n">名稱</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">初始值</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">名稱</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span>
</code></pre></div>

<ul data-connectedheader="header-7" class="a_content">
<li>
<p data-connectedheader="header-7" class="a_content">損失函數: tf.Tensor<br>
  損失函數是一個用於判斷正確程度的值，應為一個透過對占位符及模型參數作數學計算後的到的張量</p>
</li>
<li>
<p data-connectedheader="header-7" class="a_content">梯度下降器: tf.Operation<br>
  梯度下降器是用於反向修正的算法，通常不會自己寫<br>
  語法：</p>
</li>
</ul>
<div class="codehilite"><pre data-connectedheader="header-7" class="a_content"><span></span><code><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'global_step'</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">損失函數</span><span class="p">,</span> <span class="n">global_step</span><span class="p">)</span>
</code></pre></div>

<h4 id="session" data-connectedheader="header-7" class="a_content">Session建立與使用</h4>
<p data-connectedheader="header-7" class="a_content">主要訓練部分需要建立一個Session<br>
語法：</p>
<div class="codehilite"><pre data-connectedheader="header-7" class="a_content"><span></span><code><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span> <span class="c1"># 初始化變數</span>
    <span class="c1"># 主程式</span>
</code></pre></div>

<p data-connectedheader="header-7" class="a_content">sess.run有以下功能：</p>
<ul data-connectedheader="header-7" class="a_content">
<li>sess.run(Operation) 進行操作</li>
<li>sess.run(Tensor|Variable) 取出值</li>
</ul>
<p data-connectedheader="header-7" class="a_content">使用<code>sess.run(optimizer)</code>可進行訓練<br>
使用<code>sess.run(損失函數)</code>可求出損失值<br>
這兩個操作都需要輸入資料<br>
使用<code>sess.run(模型參數)</code>可取出模型參數</p>
<h4 id="_13" data-connectedheader="header-7" class="a_content">範例</h4>
<p data-connectedheader="header-7" class="a_content"><details><summary>範例：y=ax+b</p></summary>
<div class="codehilite"><pre data-connectedheader="header-7" class="a_content"><span></span><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">'font.sans-serif'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Microsoft JhengHei'</span><span class="p">]</span> <span class="c1"># 正常顯示中文</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">'axes.unicode_minus'</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># 正常顯示負號</span>
<span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">disable_eager_execution</span><span class="p">()</span> <span class="c1"># 禁用eager execution防止出RuntimeError</span>
<span class="c1">#隨機參數</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="n">random_a</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">random_b</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="c1"># 產生資料</span>
<span class="n">train_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">train_Y</span> <span class="o">=</span> <span class="n">random_a</span> <span class="o">*</span> <span class="n">train_X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">train_X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">random_b</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">train_X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">random_a</span>  <span class="c1"># y=ax+b，但是加入了噪聲</span>
<span class="c1"># 輸入值</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">"float"</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">"float"</span><span class="p">)</span>
<span class="c1"># 可訓練參數</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s2">"weight"</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s2">"bias"</span><span class="p">)</span>
<span class="c1"># 前嚮結構</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'global_step'</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># 反向改善</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">z</span><span class="p">))</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">global_step</span><span class="p">)</span>  <span class="c1"># 梯度下降</span>
<span class="c1"># 初始化所有變數</span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="c1"># 定義訓練參數</span>
<span class="n">training_epochs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">display_step</span> <span class="o">=</span> <span class="mi">2</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
    <span class="k">while</span> <span class="n">global_step</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_X</span><span class="p">)</span> <span class="o"><</span> <span class="n">training_epochs</span><span class="p">:</span>  <span class="c1"># step=執行次數/資料大小=global_step.eval()/len(train_X)</span>
        <span class="n">step</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">global_step</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_X</span><span class="p">))</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">):</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">y</span><span class="p">})</span>  <span class="c1"># 餵資料</span>
        <span class="c1"># 顯示訓練中的詳細訊息</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">display_step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">train_X</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">train_Y</span><span class="p">})</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"Epoch:"</span><span class="p">,</span> <span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"cost="</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s2">"W="</span><span class="p">,</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">W</span><span class="p">),</span> <span class="s2">"b="</span><span class="p">,</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Finished!"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"cost="</span><span class="p">,</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">train_X</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">train_Y</span><span class="p">}),</span> <span class="s2">"W="</span><span class="p">,</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">W</span><span class="p">),</span> <span class="s2">"b="</span><span class="p">,</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
    <span class="c1"># 顯示模型</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="s1">'ro'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Original data'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">W</span><span class="p">)</span> <span class="o">*</span> <span class="n">train_X</span> <span class="o">+</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Fitted line'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p data-connectedheader="header-7" class="a_content"></details></p>
<h3 id="header-8" class="a_header a_content" data-connectedheader="header-8">動態圖</h3>
<p data-connectedheader="header-8" class="a_content">動態圖基於eager execution<br>
不需要建立Session<br>
直接計算梯度並更改參數</p>
<h4 id="_15" data-connectedheader="header-8" class="a_content">動態圖架構</h4>
<p data-connectedheader="header-8" class="a_content">動態圖架構包含模型參數、損失函數、改善器</p>
<ul data-connectedheader="header-8" class="a_content">
<li>模型參數: tf.Variable<br>
  模型參數是模型中要透過訓練求得的值</li>
<li>損失函數: tf.Tensor<br>
  損失函數是一個用於判斷正確程度的值，應為一個透過對占位符及模型參數作數學計算後的到的張量<br>
  與靜態圖不同的是，損失函數是動態計算而不是在一開始就固定</li>
<li>改善器: GradientDescentOptimizer<br>
  改善器能夠透過梯度自動優化參數<br>
  語法：</li>
</ul>
<div class="codehilite"><pre data-connectedheader="header-8" class="a_content"><span></span><code><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</code></pre></div>

<h4 id="_16" data-connectedheader="header-8" class="a_content">動態圖使用</h4>
<p data-connectedheader="header-8" class="a_content">首先定義好模型參數與改善器<br>
並在過程中計算梯度<br>
<strong>計算梯度</strong></p>
<div class="codehilite"><pre data-connectedheader="header-8" class="a_content"><span></span><code><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">cost_</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">損失函數</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">cost_</span><span class="p">,</span> <span class="n">sources</span><span class="o">=</span><span class="p">[</span><span class="n">模型參數列表</span><span class="p">])</span>  <span class="c1"># 計算梯度</span>
</code></pre></div>

<p data-connectedheader="header-8" class="a_content"><strong>優化參數</strong></p>
<div class="codehilite"><pre data-connectedheader="header-8" class="a_content"><span></span><code><span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="p">[</span><span class="n">模型參數列表</span><span class="p">]),</span> <span class="n">global_step</span><span class="p">)</span>
</code></pre></div>

<p data-connectedheader="header-8" class="a_content">要取出任何的模型參數可直接使用</p>
<div class="codehilite"><pre data-connectedheader="header-8" class="a_content"><span></span><code><span class="n">參數</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div>

<p data-connectedheader="header-8" class="a_content"><details><summary>範例：y=ax+b</p></summary>
<div class="codehilite"><pre data-connectedheader="header-8" class="a_content"><span></span><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">'font.sans-serif'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Microsoft JhengHei'</span><span class="p">]</span> <span class="c1"># 正常顯示中文</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">'axes.unicode_minus'</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># 正常顯示負號</span>
<span class="c1">#隨機參數</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="n">random_a</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">random_b</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="c1"># 產生資料</span>
<span class="n">train_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">train_Y</span> <span class="o">=</span> <span class="n">random_a</span> <span class="o">*</span> <span class="n">train_X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">train_X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">random_b</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">train_X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">random_a</span>  <span class="c1"># y=ax+b，但是加入了噪聲</span>
<span class="c1"># 定義訓練參數</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s2">"weight"</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s2">"bias"</span><span class="p">)</span>
<span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'global_step'</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">getcost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>  <span class="c1"># 定義函數，計算loss值</span>
    <span class="c1"># 前嚮結構</span>
    <span class="c1">#z = tf.cast(tf.multiply(np.asarray(x, dtype=np.float32), W) + b, dtype=tf.float32)</span>
    <span class="n">z</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">))</span>  <span class="c1"># loss值</span>
    <span class="k">return</span> <span class="n">cost</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="c1"># 隨機梯度下降法作為改善器</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">training_epochs</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># 迭代訓練次數</span>
<span class="n">display_step</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">old_step</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="k">while</span> <span class="n">global_step</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_X</span><span class="p">)</span> <span class="o"><</span> <span class="n">training_epochs</span><span class="p">:</span>  <span class="c1"># 迭代訓練模型</span>
    <span class="n">step</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">global_step</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_X</span><span class="p">))</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">cost_</span> <span class="o">=</span> <span class="n">getcost</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">)</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">cost_</span><span class="p">,</span> <span class="n">sources</span><span class="o">=</span><span class="p">[</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>  <span class="c1"># 計算梯度</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="p">[</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">]),</span> <span class="n">global_step</span><span class="p">)</span>
    <span class="c1"># 顯示訓練中的詳細訊息</span>
    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">display_step</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">step</span><span class="o">!=</span><span class="n">old_step</span><span class="p">:</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">cost_</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># 損失值換為np.ndarray</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Epoch:"</span><span class="p">,</span> <span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"cost="</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="s2">"W="</span><span class="p">,</span> <span class="n">W</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s2">"b="</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">old_step</span><span class="o">=</span><span class="n">step</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">" Finished!"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"cost="</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="s2">"W="</span><span class="p">,</span> <span class="n">W</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s2">"b="</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="s1">'ro'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Original data'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">W</span> <span class="o">*</span> <span class="n">train_X</span> <span class="o">+</span> <span class="n">b</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Fitted line'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p data-connectedheader="header-8" class="a_content"></details></p>
<h3 id="header-9" class="a_header a_content" data-connectedheader="header-9">估算器</h3>
<h3 id="header-10" class="a_header a_content" data-connectedheader="header-10">keras</h3>
<p data-connectedheader="header-10" class="a_content">詳見<a href="keras">keras</a></p>
    </div>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            config: ["MMLorHTML.js"],
            jax: ["input/TeX", "output/HTML-CSS", "output/NativeMML"],
            extensions: ["MathMenu.js", "MathZoom.js"],
            enable_dollar_delimiter: true
        });
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
    <script src="https://littleorange666.github.io/script/in_view.js"></script>
    <script>
    function update_inview() {
        for (let e of document.querySelectorAll(".a_header_targeted")) {
            e.classList.remove("a_header_targeted");
        }
        let e = document.querySelector(".a_content_inview");
        if (e) {
            let t = document.getElementById(e.dataset.connectedheader);
            if (t) t.classList.add("a_header_targeted");
        }
    }
    inView(".a_content").on('enter', e => {
            e.classList.add("a_content_inview");
            update_inview();
        })
        .on('exit', e => {
            e.classList.remove("a_content_inview");
            update_inview();
        });
    let prebtn = document.getElementById("previous");
    prebtn.onclick = ()=>{
        if (location.href.endsWith("/")){
            location.href="../";
        }else{
            location.href="./";
        }
    };
    </script>
</body>

</html>